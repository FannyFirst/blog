# Streaming Joins using Few Resources

Today's post is on a topic that a lot of folks have asked for, once they dive a bit in to [Materialize](https://materialize.com).

One of our join implementation strategies uses a surprisingly small amount of additional memory: none.
"None" is a surprising amount of memory because streaming joins normally need to maintain their inputs indexed in memory.
Clearly there is a catch!

In a sense, there is.
For the plan to apply, you need to have pre-built several indexes over the involved data.
Materialize can share pre-built indexes between queries, like you might expect from a relational database, but unlike most stream processors.
Once those indexes are in place, each additional query requires no additional memory for its joins.

That is probably still not clear, and there is a lot of interesting stuff to learn, so let's get started!
By the end of the post, I hope you'll be able to put together queries that use fewer resources, and understand some of the mystery behind it.

## Materialize

Materialize is a system that allows you to express SQL queries over continually changing sources of data.
These changes are first class citizens in Materialize, rather than just "whatever happens to the data".
In particular, Materialize manipulates streams of "updates": triples `(data, time, diff)` where:
* `data` is there *where* of the update: what record changed.
* `time` is the *when* of the update: at what moment should it take effect.
* `diff` is the *what* of the update: how many copies of `data` do we add or remove.

These streams of updates describe a continually changing collection, whose contents at any time `t` are determined by adding up the updates whose `time` is less or equal to `t`.
Specifically, the collection contains as many copies of `data` as the accumulation of `diff` in those updates.
That number might be zero, in which case `data` is absent from the collection.
It probably shouldn't be a negative number, which would suggest that something has gone wrong.
It could be a large positive number which just means that there are multiple copies of `data`.

With these streams of updates, Materialize builds *dataflows* of operators that transform update streams for input collections into update streams for output collections.
Dataflows are built out of operators, and larger computations still can be formed by composing dataflows.
Ultimately, Materialize maintains multiple dataflows of updates that correctly and consistenly track the updates to arbitrary SQL views.

To do all of this, we at Materialize need to be able to build dataflow fragments that implement the various parts of SQL views.
We are going to look at the specific case of doing that for a multiway relational join.

## Relational Joins

In SQL a *relational join* of two collections is the new collection that contains all pairs of records one from each collection.
The columns of the paired records are usually concatenated, to form a collection with all of the columns present in each input.
A *multiway relational join* is this, but for any number of collections, not just two.

Folks usually don't want *all* pairs, and so joins often come with *constraints*, which are predicates that restrict down the final set of records.
Rather than produce all pairs (or triple, or quadruplets, etc), implementations will usually lean hard on the constraints to restrict their attention to the results that could possibly emerge in the output.

For example, consider this actual join fragment from TPCH query 3:
```sql
SELECT
    *
FROM
    customer,
    orders,
    lineitem
WHERE
    c_mktsegment = 'BUILDING'
    AND c_custkey = o_custkey
    AND l_orderkey = o_orderkey
    AND o_orderdate < DATE '1995-03-15'
    AND l_shipdate > DATE '1995-03-15';
```
This query calls for all triplets of data from `customer`, `orders`, and `lineitem`.
However, the query also narrows this down to records that satisfy other constraints.
Some of these constraints are on columns from just one input (*e.g.* `c_mktsegment = 'BUILDING'`).
Some of these constraints relate columns from different inputs (*e.g.* `AND c_custkey = o_custkey`).

While the constraints on single inputs reduce the data, it is the constraints on columns from different inputs that really narrows our focus.
Rather than match all records from `customer` and `orders`, we know that matches will have the same `custkey` column.
We can group each of these collections by their `custkey` column, consider pairs that match, and never consider pairs that do not match.
We've reduced down the amount of work from certainly quadratic (`|customer| x |orders|`) to something linear in the input (to group things) and the output (to enumerate each of the outputs).
This improvement can be substantial, and can be even more so as we add more relations.

As we add more relations, we would like to do the same trick.
The `lineitem` relation doesn't have a `custkey` column, and even if it did it isn't used in a constraint.
Instead, we need to think about taking the output of the binary join above, and repeating the process with the `orderkey` column.
Nothing wrong with doing that, and we end up only considering the pairs that might match on `orderkey`, which is again great news.

There are other ways we could have done the same thing.
We could have started with `lineitem` and `orders`, and then added in `customer`.
We could have started with `lineitem` and `customer`, and then added in `orders`.
The first of these is a fine idea, but the second one has some flaws.
The `lineitem` and `customer` relations don't share a constraint, so what could we use?
We'd end up taking all pairs again, which probably doesn't end up better than the other approaches (it can in some cases, but it isn't the common case).

All of this is to say: when faced with multiway relational joins, we have some options in front of us for how to perform it.
We haven't even enumerated all of the options, and they are going to become more varied as we head to streaming updates rather than static data.

## Relational Joins on Update Streams

The problem Materialize is face with is maintaining multiway relational joins over inputs presented as streams of updates.
Specifically, we need to build .. something .. that can translate input streams of updates to an output stream of updates.
That output stream of updates must have the property that at all times `t` it accumulates to the collection that is the correct answer to the multiway relational join applied to the accumulation of each of the inputs at time `t`.

One (not great) approach is to fully re-form each of the inputs at each time `t` and repeat the query to see the output, and then subtract out whatever was previously produced.
This is pretty inefficient, especially if not very much has changed.
We'd love to take advantage of the fact that we pointed at the input changes, and perhaps leap directly to the output changes.

In fact, many relational databases do this already, although not in the streaming context.

Let's consider that join from above, and ask "what if someone gave us a table `d_customer` that contained some additions to `customer`?"
Let's say we've already computed the join on the prior `customer` relation and just want to know what additions there will be in the output.
If we use the [distributive property](https://en.wikipedia.org/wiki/Distributive_property), we can conclude that
```
    customer x orders x lineitem
 +  d_customer x orders x lineitem
 =  (customer + d_customer) x orders x lineitem
```
If you believe this math, then you can see that we can go from the prior value of the join (first line) to the new value of the join (last line) by adding in a correction term that uses `d_customer` in place of `customer`.
The SQL query that figures this out the correction for us is:
```sql
SELECT
    *
FROM
    d_customer,
    orders,
    lineitem
WHERE
    c_mktsegment = 'BUILDING'
    AND c_custkey = o_custkey
    AND l_orderkey = o_orderkey
    AND o_orderdate < DATE '1995-03-15'
    AND l_shipdate > DATE '1995-03-15';
```
You might want a moment to convince yourselves that the `WHERE` constraints at the end don't change the correctness.
These constraints also distribute over `+`, so it is fine to do them on parts of an update that we then add together.

However, HOWEVER! These filters play a really interesting role now.

First, let's agree that you could have done the same thing up above with a `d_orders` or with a `d_lineitem`.
They each produce a query that would describe additions to the output from additions to the specific input.
The only differences between the queries is which of the base relations we've substituted with a `d_` relation.

The "really interesting" thing (to me, at least) is that these three queries can have different query plans.
Remember how up above the `WHERE` constraints led us to consider different ways to evaluate a query, where we started with one pair of relations, and then joined in the third?
We are going to do that again, but we can make different choices for each of the three update queries.

Generally, the `d_` update relations are smaller than their base relations.
It isn't always the case, but it is the main premise of streaming updates around instead of recomputing things from scratch.
Given that, it makes a lot of sense to start each of the three update queries with its respective `d_` relation.
Based on the *constraints*, it makes a lot of sense to follow these relations with relations they share a constraint with.

Parenthesizing to show off the intended order of joins, we are interested in doing
```
(d_customer x orders) x lineitem
(customer x d_orders) x lineitem
customer x (orders x d_lineitem)
```
The second line could have gone either way, perhaps starting with `d_orders x lineitem` instead.
However, the first and last line are different plans, and they are each the right way to respond to changes to `customer` and `lineitem` respectively.

### Update Streams?

All of the above was about handling just one batch of updates, to only one of the inputs.
Things become more interesting as we consider streams of inputs at many different `time`s, where any one `time` may contain updates to multiple inputs.
However, we are going to borrow all of the intuition up above in determining what to do.

Materialize is presented with update streams for `customer`, `orders`, and `lineitem`, and needs to build a dataflow that produces an output update stream for their join.
Let's start with the ideas from above, and see what sort of details we need to fill in.

We'll build a dataflow that has an update *path* for each of its input relations.
```
d_customer -> join_with(orders) -> join_with(lineitem)
d_orders -> join_with(customer) -> join_with(lineitem)
d_lineitem -> join_with(orders) -> join_with(customer)
```
These three paths show how to respond to each of our update streams (the `d_` names).
The paths reference the relations without the `d_` prefix, which are the accumulations of those update streams.
That is, we could replace `orders` with a fragment `d_orders -> accumulate`.
If we make those replacements, the three paths up above are defined only in terms of `d_customer`, `d_orders`, and `d_lineitem`, which we are taking to be the update streams.
If we merge all of the path outputs together, we get an update stream for the whole join, which we intend to reflect all of the input changes.

Now, does it actually do the right thing?
To answer that question we'll need to get more specific about what `join_with` does.
Spoiler: we will find out that it does not do the right thing, without a minor correction.

The `join_with` operator takes two inputs.
Coming from the left, we have `d_updates` that is an update stream we need to respond to.
We also have an accumulated collection that we drew as `d_lookup -> accumulate`, which is also an update stream but one that drives the collection we'll want to look into when updates come in from the left.
The intent, following the intuition from above, is that we should respond to updates in `d_updates` by joining with the contents of `lookup` at the `time` of the update.

For any `(data, time, diff)` from `d_updates` we are interested in the updates from `d_lookup` whose times are less or equal to `time`.
Those are the updates that are "in effect" when the `(data, time, diff)` update arrives.
Our output will just connect all of those updates with `(data, time, diff)`, pairing up columns and multiplying `diff` values.
To do this, we just need to maintain `d_lookup` updates indexed by whatever column or columns of `data` we'll be using to do our efficient constraint-based lookup.

This works great, with one exception.
If there are updates to two inputs, at the same `time`, we end up matching them twice.
We'll find the match in `d_first -> join_with(second)` and also in `d_second -> join_with(first)`.
Both of their updates are at times that are less or equal the time of the other, and show up in the output.
If we make the test "strictly less than" we end up missing the pair of updates twice, which is also wrong.

What we need to do is break the tie, which we will do by having the join inputs update in some order.
For example, we could have the `d_first` update "go first", where it will *not* see updates to `second` at the same time (but will see all updates at strictly prior times).
Then we have `d_second` go, and by this point the updates to `first` at the same time are visible.
This ends up amounting to breaking ties in `time` by some order on the join inputs themselves, which only applies for times that are exactly the same.
Fortunately, we don't need to break ties within one join input, as those updates are not matched with each other.

So that's a dataflow we can build.
But should we?

### Shared Arrangements

This dataflow plan has a number of `join_with` operators that are quadratic in the number of inputs.
Each of these operators seems to need to maintain some indexed data, so are we maintaining many copies of each input relation.
Perhaps many more than we can afford to maintain?

Yes.

This approach is pretty terrible if each of the `join_with` operators maintains their own indexed representation of the relation they perform lookups into.
