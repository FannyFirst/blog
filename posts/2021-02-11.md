# Windows enough, and Time

[Materialize](https://materialize.com) provides a SQL interface to continually changing data. You write SQL queries as if against static data, and then as your data change we keep the results of your queries automatically up to date, in milliseconds.

Materialize leans hard into the ideal that SQL is what you know best, and what you want to use to look at streaming data. At the same time, there are several tantalizing concepts that native stream processors provide that aren't obviously possible with standard SQL.

Today we'll look at how to perform time-windowed computation over temporal data.

## Temporal Data

Temporal databases are a pretty neat thing, and we aren't going to go deep on that today. Instead, let's just start with a relation that has a pretty simple schema.
```sql
-- Making a new data source.
CREATE TABLE events (
    content text,
    insert_ts numeric,
    delete_ts numeric
);
```
We have some content, clearly you could fluff that up to something more interesting, and two additional columns. The `insert_ts` and `delete_ts` columns will play the role of timestamps, indicating when an event should first appear and when it should be removed.

Of course, this is all just data right now. These are fields, and you could put whatever you want in them. You could make `delete_ts` be smaller than `insert_ts`, if you are that sort of person.

## Time-Windowed Queries

The question is now what do you do with this data?

In Materialize, you can ask questions that grab the "logical time" for your query (roughly: wall clock when you run the query) and use that in your query's logic. For example:
```sql
-- Reduce down to counts within a time interval.
SELECT content, count(*)
FROM events
WHERE mz_logical_timestamp() >= insert_ts
  AND mz_logical_timestamp()  < delete_ts
GROUP BY content;
```
This should come back with counts for each `content`, but only for records that are "valid": their `insert_ts` and `delete_ts` interval contains the current logical timestamp.

This query will change its results over time. Not just because you might add to and remove from `events`, but because `mz_logical_timestamp()` advances as you stare at your screen.

This looks like a great query! What's not to like?

The main issue really is that it is a *query*. You can ask this question over and over, but you can also ask the same thing with an arbitrary timestamp in place of `mz_logical_timestamp()`. To support that, we have to keep the entire history around. Your `events` table will grow and grow and grow.

## Time-Windowed Computation

Until recently, if you tried to create a materialized view of the query above, Materialize would tell you to take a hike. The subject of this post, at its heart, is that you can now do this.
```sql
-- Maintained collection of only valid results.
CREATE MATERIALIZED VIEW valid_events AS
SELECT content, count(*)
FROM events
WHERE mz_logical_timestamp() >= insert_ts
  AND mz_logical_timestamp()  < delete_ts
GROUP BY content;
```
What's all this then?

Presumably `valid_events` has the property that if you `SELECT` from it you should see the same results as for the time-windowed `SELECT` in the previous section. That is 100% true.

What is also true is that `valid_events` has enough information from you, in the form of the query itself, to only maintain enough historical detail to answer these questions from now going forward. Once `mz_logical_timestamp()` passes a record's `delete_ts` it cannot be seen, and soon after will no longer be stored either. The in-memory footprint of `valid_events` stays bounded by the number of records that could still satisfy this constraint (those records that are currently valid, or who may yet become valid in the future).

## A Brief Example

Let's do some testing with our table and maintained view. Tables have the nice property that we can interactively update them from within Materialize, rather than spinning up a Kafka cluster.

Let's start with something simple: we'll just look at the records currently present in our `valid_events` view. Let's define a different view to do that, though, without the aggregation so you can see the raw data:
```sql
-- Maintained collection of only valid results.
CREATE MATERIALIZED VIEW valid AS
SELECT content, insert_ts, delete_ts
FROM events
WHERE mz_logical_timestamp() >= insert_ts
  AND mz_logical_timestamp()  < delete_ts
```
We'll print out the things in our view, along with the current logical timestamp.
```text
materialize=> SELECT *, mz_logical_timestamp() FROM valid;
 content | insert_ts | delete_ts | mz_logical_timestamp
---------+-----------+-----------+----------------------
(0 rows)

materialize=>
```
Now let's put some data in there. I'm going to just take advantage of the fact that `INSERT` statements can also use `mz_logical_timestamp()` to populate the data with some records that last five seconds.
```
materialize=> insert into events VALUES ('hello', mz_logical_timestamp(), mz_logical_timestamp() + 5000);
materialize=> insert into events VALUES ('hello', mz_logical_timestamp(), mz_logical_timestamp() + 5000);
materialize=> insert into events VALUES ('hello', mz_logical_timestamp(), mz_logical_timestamp() + 5000);
```
Each of these were executed by me, a human, and so almost certainly got different `insert_ts` and `delete_ts` timestamps.

I then typed *incredibly fast* to see the output for the query:
```text
materialize=> SELECT *, mz_logical_timestamp() FROM valid;
 content |   insert_ts   |   delete_ts   | mz_logical_timestamp
---------+---------------+---------------+----------------------
 hello   | 1613084609890 | 1613084614890 |        1613084613168
 hello   | 1613084611459 | 1613084616459 |        1613084613168
 hello   | 1613084610799 | 1613084615799 |        1613084613168
(3 rows)
```
We can see that the `insert_ts` and `delete_ts` values are indeed `5000` apart, and for each of the outputs the `mz_logical_timestamp` lies between the two. What happens if we type the query again, very quickly?
```text
materialize=> SELECT *, mz_logical_timestamp() FROM valid;
 content |   insert_ts   |   delete_ts   | mz_logical_timestamp
---------+---------------+---------------+----------------------
 hello   | 1613084609890 | 1613084614890 |        1613084613988
 hello   | 1613084611459 | 1613084616459 |        1613084613988
 hello   | 1613084610799 | 1613084615799 |        1613084613988
(3 rows)
```
The `mz_logical_timestamp` values have increased ..
```text
materialize=> SELECT *, mz_logical_timestamp() FROM valid;
 content |   insert_ts   |   delete_ts   | mz_logical_timestamp
---------+---------------+---------------+----------------------
 hello   | 1613084609890 | 1613084614890 |        1613084614843
 hello   | 1613084611459 | 1613084616459 |        1613084614843
 hello   | 1613084610799 | 1613084615799 |        1613084614843
(3 rows)
```
.. and increased again ..
```text
materialize=> SELECT *, mz_logical_timestamp() FROM valid;
 content |   insert_ts   |   delete_ts   | mz_logical_timestamp
---------+---------------+---------------+----------------------
 hello   | 1613084611459 | 1613084616459 |        1613084615628
 hello   | 1613084610799 | 1613084615799 |        1613084615628
(2 rows)
```
.. and we lost one! Now that `mz_logical_timestamp` has passed `1613084614890` that record no longer satisfies the predicate, and is no longer present.
```text
materialize=> SELECT *, mz_logical_timestamp() FROM valid;
 content |   insert_ts   |   delete_ts   | mz_logical_timestamp
---------+---------------+---------------+----------------------
 hello   | 1613084611459 | 1613084616459 |        1613084616392
(1 row)
```
One more has dropped out.
```text
materialize=> SELECT *, mz_logical_timestamp() FROM valid;
 content | insert_ts | delete_ts | mz_logical_timestamp
---------+-----------+-----------+----------------------
(0 rows)
```
Ah, they are all gone. My fingers can rest now.

Although this looks rather similar to re-typing the `SELECT` query that explicitly filters against `mz_logical_timestamp()`, the difference here is that everything is dataflow with updates flowing through it. If we were to `TAIL` the view, we would see exactly the moments at which the collection changes, without polling the system repeatedly.

## Windows: Sliding and Tumbling

The pattern we saw above was actually very powerful: records could state both their insertion and deletion times. If a record wants to be around for 10s it can do that, if it wants to stay for a year or forever, it could probably do that too.

But, let's check out some other idioms.

**Sliding windows** are fixed-size time intervals that you drag over your temporal data, providing results that change as time moves forward. They are great for getting out a view of your recent data, and allow you to control what that looks like.
```sql
-- Slide a 5 second window over temporal data.
CREATE MATERIALIZED VIEW valid_events AS
SELECT content, count(*)
FROM events
WHERE mz_logical_timestamp() >= insert_ts
  AND mz_logical_timestamp()  < insert_ts + 5000
GROUP BY content;
```
Here we've changed the query ever so slightly, to ignore the records `delete_ts` field and just impose an upper bound of five seconds after the insertion. This ensures that even silly records will get cleaned up soon enough.

Importantly, the `insert_ts` field can be whatever you want. It is in milliseconds, and your output sliding window will update up to 1,000 times per second. This is called a "continual slide" window. You aren't obliged to have the 5 second window hop only on second boundaries, or anything like that.

I mean, you could, if that is what you want:
```sql
-- Slide a 5 second window over temporal data, second-by-second.
CREATE MATERIALIZED VIEW valid_events AS
SELECT content, count(*)
FROM events
WHERE mz_logical_timestamp() >= 1000 * (insert_ts / 1000)
  AND mz_logical_timestamp()  < 1000 * (insert_ts / 1000) + 5000
GROUP BY content;
```

**Tumbling windows** (sometimes: "hopping") are just those coarse-grained sliding windows that slide one unit at a time. Each record contributes to only one window.
```sql
-- Slide a 5 second window over temporal data, second-by-second.
CREATE MATERIALIZED VIEW valid_events AS
SELECT content, count(*)
FROM events
WHERE mz_logical_timestamp() >= 1000 * (insert_ts / 1000)
  AND mz_logical_timestamp()  < 1000 * (insert_ts / 1000) + 1000
GROUP BY content;
```
I think they have a special name because they are much easier to implement for non-streaming systems. They are also useful if you want to see aggregate values that can just be added up to get an hourly total, or daily or whatever. The sliding windows over-count the contributions of each record.

## Going Beyond Count

Perhaps this is obvious, but you can do more than just `count(*)` things. The `valid` view we produced up above, containing all currently valid events, is just like any other materialized view in Materialize, and you can use it as you like. Join it with other temporal relations, put it in a correlated subquery, feed it in to an exotic `jsonb` aggregation.

This is the main difference between what is going on here, and [time-series databases](https://en.wikipedia.org/wiki/Time_series_database) (TSDBs). TSDBs are good at storing historical measurements and serving them up when you ask, but they aren't generally as good at maintaining non-trivial computation over changing data. They can usually handle counts and sums pretty well, but if you want to maintain complex views over your changing, temporal data I recommend trying out Materialize.

Let's do a quick example with some non-trivial joins.

The [TPC-H benchmark](http://www.tpc.org/tpch/) is a collection of 22 decision support queries. To pick one at random, query 3 looks like
```sql
SELECT
    o_orderkey,
    o_orderdate,
    o_shippriority,
    sum(l_extendedprice * (1 - l_discount)) AS revenue
FROM
    customer,
    orders,
    lineitem
WHERE
    c_mktsegment = 'BUILDING'
    AND c_custkey = o_custkey
    AND l_orderkey = o_orderkey
    AND o_orderdate < DATE '1995-03-15'
    AND l_shipdate > DATE '1995-03-15'
GROUP BY
    o_orderkey,
    o_orderdate,
    o_shippriority
ORDER BY
    revenue DESC,
    o_orderdate
LIMIT 10;
```
This query determines the top ten unshipped orders by value as of some date (here: `'1995-03-15'`). Wouldn't it be neat to compute it for all dates, continually, as it happens?

Let's imagine changing those two lines, and see what we get.
```sql
...
    AND o_orderdate < mz_logical_timestamp()
    AND l_shipdate > mz_logical_timestamp()
...
```
The first changed constraint restricts our attention to orders placed before "now", which has the effect of keeping orders out of the query until we've reached their `o_orderdate` column.
In a real setting, this is probably a bit of a no-op, in that the record probably lands in our input stream around the order date anyhow.
<!-- For our experiment it will be helpful to keep it, because I'm going to load the data in bulk rather than go back in time to 1995 and stream the data in over seven years; this will prevent all orders from being active immediately. -->

The second changed constraint restricts our attention to `lineitem` records that have not shipped by "now". This has the effect of deleting the record once we reach `l_shipdate`, effectively garbage collecting that relation for us, which is nice given that `lineitem` is the largest of the input relations (it is a "fact table").

Expressed this way, with temporal filters, the memory footprint of the view will be proportional to the sizes of `orders` and `customer`, plus as much of `lineitem` is present but has not yet shipped. We could even add in a constraint that orders are not *too* old, if we wanted to tighten our memory belt and focus in on recent shipping issues.

## Conclusions

There are some limitations. I should have mentioned this earlier.

You can only use `mz_logical_timestamp()` in `WHERE` clauses, where it must be directly compared to expressions not containing `mz_logical_timestamp()` or in a conjunction (`AND`) with other clauses like that. You aren't allowed to use `!=` at the moment, but clever folks could figure out how to fake that out. For the reasoning on all this, check out the implementation discussion next!

Limitations notwithstanding, I'm personally very excited about these temporal filters. They open up the door to functionality and behaviors streaming systems provide only with special language extensions. But, all you really need is SQL, and the ability to refer to time, to make your data run.

---

Get access to Materialize [here](https://materialize.com/download/). You'll need to build from source and use the `--experimental` flag to try it out at the moment, but it should be available soon in an upcoming release. In the meantime, take a swing by [the Materialize blog](https://materialize.com/blog/) for more cutting-edge content, and join the community Slack through the bright banner at the top of the [Materialize homepage](https://materialize.com/).

---

## Appendex: Implementation

Some of you are surely here to hear how the magic works.

The magic lives in `filter.rs`, which is the Rust source for our filter operator. Normally, the filter logic is very simple, and evaluates predicates against records and drops those that do not pass the predicate. That code was sufficiently simple that it did not previously merit its own file (10 lines of code, roughly).

However, this all changed with temporal filters, which need to do something more clever than just drop or retain updates. Let's talk through what they need to do first, before we see how they go about doing it.

In [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow), which lies in wait underneath Materialize, dataflow operators consume and produce *updates*: triples of `(data, time, diff)`. The `data` is the data payload: the values in the columns of your individual records. The `time` is the logical timestamp at which the change should take effect. The `diff` is .. a signed integer let's say, that says how the occurence count of `data` should change: postitive numbers indicate additions, negative numbers indicate deletions.

The traditional (non-temporal) filter responds to `(data, time, diff)` triples by applying a predicate to `data`, and either dropping or retaining the record based on what it sees.

The less-traditional temporal filter looks for predicates of the form
```
mz_logical_timestamp() CMP_OP EXPRESSION
```
where `CMP_OP` is a comparison operation (not `!=`) and `EXPRESSION` is an expression that does not contain `mz_logical_timestamp()`. Roughly, the expression is a function of `data`, and once we evaluate it we get a bound on `mz_logical_timestamp()`. If we have a bunch of comparisons, none `!=`, we end up with maybe lower and and maybe upper bounds, describing an interval of time.

An update `(data, time, diff)` takes effect at `time` and is in then effect indefinitely. However, we can narrow its range of time to `[lower, upper)` by transforming the input update to two output updates:
```
(data, max(time, lower), diff)
(data, upper, -diff)
```
Ignoring corner cases for the moment, this delays the introduction of the record until at least `lower`, and come `upper` the record is deleted.

There are a variety of corner cases to double check, mostly around what to do if a bound is absent, or if they cross (you wrote the query and supplied the data; we can't rely on things making sense). You'll want to double check that the above makes sense when `diff` is negative (a deletion undoes the window its insertion would have introduced). We also need to update our query optimizer as filters can now do slightly weirder things than they could before.

But actually, the above is basically the implementation. The whole file comes in at under 300 lines, and that's with comments and a copyright header.