# Change Data Capture

**DRAFT**

At Materialize we traffic in computation over data that change. As a consequence, it is important to have a way to write down and read back changes to data. An *unambiguous*, *robust*, and *performant* way to write down and read back changes to data.

What makes this challenging? Why not just write out a log of whatever happens to the data?

If you are familiar with the modern stream storage system---Kafka, Kinesis, Pulsars, Azure Event Hubs---you may know how awkwardly our three desiderata interact. If you want performance, you should expect to read concurrently from streams that are not totally ordered. If you want robustness you'll need to be prepared for duplication in your streams as at-least-once stream storage systems cope with anomalies. Life can be pretty hard if you want correct answers in a distributed setting.

This post will talk through our first revision of our *change data capture* format: how we write down and read back changes to data. This format allows arbitrary duplication and reordering of its messages, among other anomalies, while maintaining a compact footprint. These features allow us to use streaming infrastructure that does not protect against these issues (most of them), but also to introduce several new benefits which we will get to by the post's end.

## Changing Data

Let's start out by looking at the problem we have to deal with.

Materialize maintains computations over data that change. The data are large relational collections whose individual records may come and go for reasons we can't anticipate or constrain. All we know is that as time progresses, the data change.

We can record the changes that happen to the data by writing down the records that are added to and removed from the collection at each time. In Materialize we do this with "update triples" which contain 1. the record that change, 2. the time it changed, and 3. how it changed (added, or removed).

Let's take an example:
```
// three records at time0
(record0, time0, +1)
(record1, time0, +1)
(record2, time0, +1)

// one record is updated
(record1, time1, -1)
(record4, time1, +1)

// two records are deleted
(record0, time2, -1)
(record4, time2, -1)
```

We see a sequence of changes, ordered by the times `time0`, `time1`, and `time2`. At `time0` we see three records are introduced. We don't know if these are the "initial data", but we don't need to; we are just transcribing the changes that occur. At `time1` we see both a deletion and an addition of a record. We can interpret this (perhaps incorrectly) as an update to `record1` that changes it to `record4`; again we aren't worried about the reason for the change, just what the change was. Finally, at `time2` we see the deletion of two records.

This is the sort of information we want to record. An evolving history of what happens to our data, as time advances.

In fact we want to know just a bit more. In the transcript up above we see changes through `time2`, but we don't actually know that we won't see another update at `time2`. Alternately,
perhaps the history is correct even through some `time3` > `time2`. How do we communicate that the absence of information is information itself, without accidentally implying that there are no more changes coming at all?

Our history has *updates to data*, but we can also provide *progress statements* that advance the clock of times that are still due to come. These statements have a few names depending on your background: watermarks, punctuation, frontiers. We'll just use a simple statement `finish time` to mean that all of the updates for `time` and any earlier times have now been presented.

Here is a history with progress statements.
```
update (record0, time0, +1)
update (record1, time0, +1)
update (record2, time0, +1)
finish time0
update (record1, time1, -1)
update (record4, time1, +1)
finish time1
update (record0, time2, -1)
update (record4, time2, -1)
finish time2
finish time3
...
```

## Anomalies and Opportunities

Because computer systems are complicated, we can't simply write down the history above and expect everything to work out. Because computer systems are complicated, they will occasionally re-order or duplicate records. Mostly, this is because people want to use more than one computer, and as soon as you start doing that the two computers rarely agree on how things were supposed to happen.

Anyhow, modern stream storage like Kafka, Kinesis, Pulsar, and Azure Event Hubs have quirks that mean if you want performance and fault-tolerance, your data might get shuffled around and duplicated. On the plus side, you are pretty sure that what you write will be recorded and eventually available to readers.

It probably wouldn't take much to convince you that the history as written above loses something when you reorder and duplicate rows in it. If you repeat an `update` statement, you might believe that you should do the update twice. If you reorder a `progress` statement, you may prematurely conclude that the data have ceased changing and miss some updates. The format as presented above is pretty easy to read, but it isn't great when uncooperative computers get their hands on it and change its layout.

We'll want a format that protects us against these vagaries of uncooperative computers.

At the same time, building an immunity to reordering and duplication provides us with some opportunities.

* If we devise a representation that can be written and read in any order, then we can deploy a large number of concurrent writers and readers. We may be able to grind through a history much more quickly as a result.

* If we devise a representation that can tolerate arbitrary duplication, then we can use multiple computers to reduntantly compute and communicate the data. This provides so-called *active-active* fault-tolerance, which duplicates work but insures against the failure (or planned outage) of individual workers.

There are several other advantages to these degrees of robustness, from resharding streams of data to easing the reintroduction of new or recovered replicas. Migrating between query plans without interruption. Stuff like that.

## Materialize CDCv2

Let's get right in to the proposal.

### A Sketch

We will make two types of statements, both of which will be statements true about the final history. We will make these statements only when we are certain they are true, and they will remain true from that point on.

* *Update statements* have the form `update (data, time, diff)` and indicate that the change that `data` undergoes at `time` is exactly `diff`.

* *Progress statements* have the form `progress (time, count)` and report the number of distinct updates that occur at `time`.

A stream of these statements may be arbitrarily duplicated and reordered, and we can still recover as much of the history as is fully covered by the update and progress statements. In addition, we get the often overlooked property that the memory footprint of the recovery process can be proportional only to those statements that have been received for times that have not yet been recovered; we do not need to maintain unboundedly long histories.

We will work through the details of the recovery process, but ideally the intuition is clear about why this might work. As we collect statements, we start to fill in the puzzle of what the history looks like. We can place updates at moments in time, and we learn how many it takes for the set of updates for a time to be complete. We can perform this work even if statements come out of order, and statements about information we already know can just be discarded.

As we see new updates we can report them immediately. As we complete the set of updates for a time, we can drop the updates we retained for deduplication and replace them with a marker that this time is complete. As intervals of time complete we can replace them with their upper and lower bounds.

### An Implementation

First off, we are going to change the structure of statements to run a bit more lean. Update statements will contain batches of updates, and progress statements will be about intervals of times. The specific Rust types that I am use are
```rust
/// A message in the CDC V2 protocol.
enum Message<Data, Time, Diff> {
    /// A batch of update statements.
    ///
    /// Each statement contains a datum, a time, and a difference, and asserts
    /// that the multiplicity of the datum changes at the time by the difference.
    Updates(Vec<(Data, Time, Diff)>),
    /// A statement about the number of updates within a time interval.
    Progress(Progress<Time>),
}
```
Clearly we've just deferred the complexity of the progress messages. Here it is.
```rust
/// An statement about the number of updates at times within an interval.
///
/// This statement covers all times beyond `lower` and not beyond `upper`,
/// and records in `counts` all of the times with non-zero counts.
struct Progress<Time> {
    /// The lower bound of times contained in this statement.
    pub lower: Vec<Time>,
    /// The upper bound of times contained in this statement.
    pub upper: Vec<Time>,
    /// All non-zero counts for times beyond `lower` and not beyond `upper`.
    pub counts: Vec<(Time, usize)>,
}
```
The `lower` and `upper` bounds are each vectors of time, which might seem odd at first. In our world, times aren't necessarily *totally* ordered, and an interval of time is better explained by *sets* of times, two of them, where the interval contains those times that are greater or equal to an element of `lower` and greater or equal to no elements of `upper`. You are welcome to think of them as integers, but bear in mind that `upper` could be empty (which is how one indicates an interval that ends a stream).

We'll work through how one might implement this in a timely dataflow system, but let's start with the simpler task of re-ordering and deduplicating an arbitrarily mangled input stream of `Message` records.

The iterater that I've written wraps around an arbitrary `I: Iterator<Item = Message>` which is Rust's way of saying "any specific type that can produce a sequence of `Message` items". It also wraps a bit of additional state as well, used to keep track of what we've seen. I've left the comments in, but if everything looks intimidating there are just six fields.

```rust
struct Ordered<I, D, T, R>
where
    I: Iterator<Item = Message<D, T, R>>,
    T: Hash+Ord+Lattice+Clone,
    D: Hash+Eq,
    T: Hash+Eq,
    R: Hash+Eq,
{
    /// Source of potentially duplicated, out of order cdc_v2 messages.
    iterator: I,

    /// Updates that have been received, but are still beyond `reported_frontier`.
    ///
    /// These updates are retained both so that they can eventually be transmitted,
    /// but also so that they can deduplicate updates that may still be received.
    updates: std::collections::HashSet<(D, T, R)>,

    /// Frontier through which the iterator has reported updates.
    ///
    /// All updates not beyond this frontier have been reported.
    /// Any information related to times not beyond this frontier can be discarded.
    ///
    /// This frontier tracks the meet of `progress_frontier` and `messages_frontier`,
    /// our two bounds on potential uncertainty in progress and update messages.
    reported_frontier: Antichain<T>,

    /// Frontier of accepted progress statements.
    ///
    /// All progress message counts for times not beyond this frontier have been
    /// incorporated in to `messages_frontier`. This frontier also guides which
    /// received progress statements can be incorporated: those whose for which
    /// this frontier is beyond their lower bound.
    progress_frontier: Antichain<T>,

    /// Counts of outstanding messages at times.
    ///
    /// These counts track the difference between message counts at times announced
    /// by progress messages, and message counts at times received in distinct updates.
    messages_frontier: MutableAntichain<T>,

    /// Progress statements that are not yet actionable due to out-of-orderedness.
    ///
    /// A progress statement becomes actionable once the progress frontier is beyond
    /// its lower frontier. This ensures that the [0, lower) interval is already
    /// covered, and that we will not leave a gap by incorporating the counts
    /// and reflecting the progress statement's upper frontier.
    progress_queue: Vec<Progress<T>>,
}
```

I thought for demonstration purposes I would have the iterator produce the `update` and `finish` statements we had back in the simple history. For reasons, I'd rather produce a batch of updates and one `finish` statement, all at the same time (it is easier to do that once, than to trickle out updates one by one; you need another state machine for that).
```rust
impl<D, T, R, I> Iterator for Ordered<I, D, T, R>
where
    I: Iterator<Item = Message<D, T, R>>,
    T: Debug+Hash+Ord+Lattice+Clone,
    D: Debug+Hash+Eq+Clone,
    R: Debug+Hash+Eq+Clone,
{
    // Produces pairs of update batches, and the next finished frontier.
    type Item = (Vec<(D, T, R)>, Antichain<T>);
    fn next(&mut self) -> Option<Self::Item> {
        // Not written yet!
        unimplemented!()
    }
}
```

This is the structure of what we'll need to write: each time someone asks, we repeatedly interrogate the wrapped `iterator` until we realize that we've learned enough to produce a new announcement about updates that are now finished. It should then be a simple transformation to make it "push" instead of "pull", reacting to new messages sent to it.

We'll sketch out the body of the `next` method, leaving a few bits of logic undeveloped for the moment.
The main thing we'll do in this cut is to process each received message, either `Updates` or `Progress`, and then call out what we'll need to do to afterwards. In fact, we can do the message receipt a few times if we want; we don't have to take a break for the clean up logic for each message.
```rust
    // Body of `next(&mut self) -> Option<Self::Item>.
    // Standard idiom: iterate until we can return, and bail with `None` if we run out.
    while let Some(message) = self.iterator.next() {
        match message {
            Message::Updates(mut updates) => {
                // Discard updates at reported times, or duplicates at unreported times.
                updates.retain(|dtr| self.reported_frontier.less_equal(&dtr.1) && !self.updates.contains(dtr));
                // Decrement our counts of novel and now-accounted-for messages.
                self.messages_frontier.update_iter(updates.iter().map(|(_,t,_)| (t.clone(), -1)));
                // Record the messages in our de-duplication collection.
                self.updates.extend(updates.into_iter());
            },
            Message::Progress(progress) => {
                // A progress statement may not be immediately actionable.
                self.progress_queue.push(progress);
            }
        }

        // Drain actionable progress messages.
        unimplemented!()

        // Determine if the lower bound of progress_frontier and updates_frontier has advanced.
        // If so, we can determine and return a batch of updates and an newly advanced frontier.
        unimplemented!()
    }
    // If we've exhausted our iterator, we have nothing to say.
    None
```
The only real work happens received `Updates`, where we discard any updates that are 1. for times that we have already resolved, or 2. are already present in our cache. Surviving updates result in a decrement for the expected count at that time (even if the expected count is not postive; that message might come later), and get stashed to help with future deduplication.

The two remaining bits of logic are how to integrate progress statements, which require some care because there may be gaps in our timeline, and how to close out intervals of time if that is appropriate, which also requires some care.

We'll start with integrating progress statements.
```rust
    // Drain all actionable progress messages.
    // A progress message is actionable if `self.progress_frontier` is beyond the message's lower bound.
    while let Some(position) = self.progress_queue.iter().position(|p| <_ as PartialOrder>::less_equal(&AntichainRef::new(&p.lower), &self.progress_frontier.borrow())) {
        // Extract progress statement.
        let mut progress = self.progress_queue.remove(position);
        // Discard counts that have already been incorporated.
        progress.counts.retain(|(time,_count)| self.progress_frontier.less_equal(time));
        // Record any new reports of expected counts.
        self.messages_frontier.update_iter(progress.counts.drain(..).map(|(t,c)| (t,c as i64)));
        // Extend self.progress_frontier by progress.upper.
        let mut new_frontier = timely::progress::Antichain::new();
        for time1 in progress.upper {
            for time2 in self.progress_frontier.elements() {
                new_frontier.insert(time1.join(time2));
            }
        }
        self.progress_frontier = new_frontier;
    }
```
Although this may look a bit beastly, it isn't all that terrible. The `while` loop iterates as long as we can find a progress statement whose lower bound is less or equal to our current `self.progress_frontier`; this ensures that we can cleanly graft the progress statement on to what we currently have. We then extract that statement, discard any counts for times that have been resolved, incorporate any remaining statements as expected counts, and then extend `self.progress_frontier` to cover the upper bound of the progress statement. And then repeat, until we can't find a progress statement that can be cleanly grafted.