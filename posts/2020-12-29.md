
# Slicing up temporal aggregates with Materialize

Materialize computes and maintains SQL queries as your underlying data change.
This makes it especially well-suited to tracking the current state of various SQL queries and aggregates!

But, what if you want to root around in the past?
Maybe you want to compare today's numbers to *yesterday*'s numbers.
Maybe you want to scrub through the past, moving windows around looking for the most interesting moments!

Today, we'll build up one way to use Materialize to explore temporal data.
As is often the case, we'll write things in vanilla SQL, but take advantage of Materialize's unique performance to build surprisingly reactive applications.

---

## A source of temporal data

You've probably got some data with timestamps in it.

I'm going to use a fairly common [data set of NYC taxi rides](https://github.com/toddwschneider/nyc-taxi-data).
You are welcome to grab it too, but it is fairly large.
Feel free to grab a small subset, or just follow along for now.

Each record in this data represents a ride, and has a pick up and drop off time.
Let's take the drop off time as the "event time" for now, as this is presumably when the data (including fare, distance, etc) was finalized.

The query I'm used to folks using on this data looks like so,
```sql
SELECT
    passenger_count,
    MIN(fare_amount),
    MAX(fare_amount)
FROM
    tripdata
GROUP BY
    passenger_count
```
This query determines for each `passenger_count` (number of folks in the taxi) the minimum and maximum fares paid.
The query aggregates across *all* of the data, because that's what's going to do the most work.
But, this isn't always (or even *usually*) what folks want.

Let's imagine instead that what *you* want is to subset the data by some time interval.
```sql
SELECT
    passenger_count,
    MIN(fare_amount),
    MAX(fare_amount)
FROM
    tripdata
WHERE --                          /- your arguments -\
    tpep_dropoff_datetime BETWEEN <TIME1> AND <TIME2>
GROUP BY
    passenger_count
```
This query does the same aggregation, but over a restricted amount of data that may be more meaningful to you.
Perhaps the data corresponds to some month you are investigating.
Perhaps you are scrubbing around through time looking for the moments of greatest disparity.
In any case, you aren't interested in just the one aggregate across all of the data.

## A zeroth approach: That query

That query up there works fine.
Materialize can compute and efficiently maintain it for you.
The problem is that you might want to *change* `TIME1` or `TIME2` (or both!).
That would make it a brand new query, and Materialize would need to start from scratch.
That's possibly not the experience you were hoping for.

If you are in an organization where multiple people are looking at the same data, it would be annoying to have to timeshare access to the query.
Ideally, we'd be able to let you and others supply pairs `(TIME1, TIME2)` as data, and get results streamed out at you.
We'll go further and commit to a specific schema:
```sql
CREATE TABLE queries (key int8, time1 TIMESTAMP, time2 TIMESTAMP);
```
Here the `key` is what identifiers *you*, and the `time1` and `time2` columns are your parameters of interest.
You (and others) can *change* this relation, adding in new queries and removing those no longer of interest.
We'd like to build up a query in Materialize that allows these live changes, and gives interactive results.

## A first approach: (Lateral) Joins

If you've been here before, you may have a hunch that LATERAL joins are a candidate solution.
If not, check out this [blog post on lateral joins](https://materialize.com/lateral-joins-and-demand-driven-queries/).
It turns out that we can write a query that joins `queries` with our `SELECT` up above to produce the results we want:
```sql
SELECT *
FROM
    queries,
    LATERAL (
        SELECT
            passenger_count,
            MIN(fare_amount),
            MAX(fare_amount)
        FROM
            tripdata
        WHERE --                      /- now from queries -\
            tpep_dropoff_datetime BETWEEN TIME1 AND TIME2
        GROUP BY
            passenger_count
    )
```
The `LATERAL` keyword exposes the columns of `queries` to the subquery that follows.
In particular, it allows correlation with the `TIME1` and `TIME2` columns of `queries`.
The result is a correlated subquery that produces independent results for each pair of bindings.

This is syntactically and semantically great!
It does exactly what we want, and is very concise.
Unfortunately, it is also very inefficient.

The problem is that Materialize cannot discern any common structure between the subqueries.
There is nothing obvious to re-use between them.
Each new row in `queries` will effectively prompt as much computation as a from-scratch query.

Lateral joins can work great when the parameter bindings narrow the computation, for example when they populate equality constraints.
When the parameter bindings are used in other constraints, it becomes much less obvious how to share the computation and state of the subqueries.

## A second approach: time slicing

While naive lateral joins could not identify commonality between subqueries, it *does* exist.

For example, imagine all `TIME1` and `TIME2` bindings were cleanly on the hour.
We could perform the aggregation above for each hour.
```sql
SELECT
    passenger_count,
    date_trunc('hour', tpep_dropoff_datetime),
    MIN(fare_amount),
    MAX(fare_amount)
FROM
    tripdata
GROUP BY
    passenger_count,
    date_trunc('hour', tpep_dropoff_datetime)
```
From this reduced data, each of the queries could pick up and stitch together their hours of interest.
The reduction down to hours was common across all queries, although each has its own unique work to do assembling the aggregates.

What if the times aren't on the hours but are on minutes instead?
```sql
SELECT
    passenger_count,
    date_trunc('minute', tpep_dropoff_datetime),
    MIN(fare_amount),
    MAX(fare_amount)
FROM
    tripdata
GROUP BY
    passenger_count,
    date_trunc('minute', tpep_dropoff_datetime)
```
Now we have aggregates at the granularity of minutes. We can do seconds too!

You may think we've made life harder because there are so many more aggregates to put back together.
There are 60 times as many minutes as there are hours, including 10,080 in a week long interval.
That is a lot more work to do, and it goes up if we did those aggregates at second boundaries.

However, no one said you had to use only seconds. Or only minutes.
You can cover most of your hypothetical week with hourly aggregates, and then just grab a few minutely aggregates at the very edges, and a few secondly aggregates if you want those too.

Let's spell this out with an example, as it will be important to be clear.
Let's say your times are
```
|   key |               time1 |               time2 |
|------:|--------------------:|--------------------:|
| 12345 | 2020-12-29 12:53:58 | 2020-12-30 11:01:03 |
```
If we want to collect aggregates that cover the span from `time1` to `time2`, we can do that with the following intervals.
Notice that all of these intervals are either a second, a minute, or (suppressed) an hour.
```
|   key |               time1 |               time2 |
|------:|--------------------:|--------------------:|
| 12345 | 2020-12-29 12:53:58 | 2020-12-29 12:53:59 |
| 12345 | 2020-12-29 12:53:59 | 2020-12-29 12:54:00 |
| 12345 | 2020-12-29 12:54:00 | 2020-12-29 12:55:00 |
| 12345 | 2020-12-29 12:55:00 | 2020-12-29 12:56:00 |
| 12345 | 2020-12-29 12:56:00 | 2020-12-29 12:57:00 |
| 12345 | 2020-12-29 12:57:00 | 2020-12-29 12:58:00 |
| 12345 | 2020-12-29 12:58:00 | 2020-12-29 12:59:00 |
| 12345 | 2020-12-29 12:59:00 | 2020-12-30 00:00:00 |
| 12345 | 2020-12-30 00:00:00 | 2020-12-30 01:00:00 |
  ... 10 hours later ...
| 12345 | 2020-12-30 11:00:00 | 2020-12-30 11:01:00 |
| 12345 | 2020-12-30 11:01:00 | 2020-12-30 11:01:01 |
| 12345 | 2020-12-30 11:01:01 | 2020-12-30 11:01:02 |
| 12345 | 2020-12-30 11:01:02 | 2020-12-30 11:01:03 |
```
It turns out that these were relatively concise times.
Generally, you might expect ~30-ish minutes and seconds on each end.
But, not any intractable number.

---

So, let's imagine for now that you've provided your input in this more expansive representation.
How might you get your data out?
What query do we need to write to make that happen?

We need to take our hourly, minutely, and secondly aggregates and turn them in to intervals.
This is no more complicated than
```sql
SELECT
    passenger_count,
    time as time1,
    time + INTERVAL '1 hour' as time2,
    min_fare_amount,
    max_fare_amount
FROM (
    SELECT
        passenger_count,
        date_trunc('hour', tpep_dropoff_datetime) as time,
        MIN(fare_amount) as min_fare_amount,
        MAX(fare_amount) as max_fare_amount
    FROM
        tripdata
    GROUP BY
        passenger_count,
        date_trunc('hour', tpep_dropoff_datetime)
)
```
We now have the data written down as a key (`passenger_count`), an interval (`time1` and `time2`), and the aggregate values (`min_fare_amount` and `max_fare_amount`).
We can repeat this for minutes and seconds, and put all records in the same collection
```sql
CREATE VIEW all_intervals AS
SELECT * FROM hourly_aggregates UNION ALL
SELECT * FROM minutely_aggregates UNION ALL
SELECT * FROM secondly_aggregates
```
We know that the measurements won't clash, because they are distinct in each input, and intervals from different inputs have different widths.

Now, we can just join `queries` and `all_intervals`, and aggregate out the various time intervals to get accumulated results.
```sql
SELECT
    key,
    passenger_count,
    MIN(min_fare_amount),
    MAX(max_fare_amount)
FROM
    queries,
    all_intervals
WHERE
    queries.time1 = all_intervals.time1 AND
    queries.tiem2 = all_intervals.time2
GROUP BY
    key,
    passenger_count
```

## Appendix: Automating the horrible slicing

We asked the user to provide pre-sliced intervals.
That seems error-prone.
Surely we can do that for them?

Indeed we can, but my version is pretty terrible to read. Perhaps it can be improved.

So, let's imagine now that we just have a relation with `(key, time1, time2)` triples, and no requirement that they be aligned to hours, minutes, or seconds.
We need to peel out some seconds near `time1`, then some minutes, then some hours, then some minutes, and then some seconds ending at `time2`.

The logic to do this isn't impossible, just wordy.
Here's what I wrote for the "seconds near `time1`":
```sql
-- Try each of the 60 seconds near `time1`;
-- accept if it describes an interval beyond `time1`,
-- and not beyond `time2`.
CREATE VIEW time1_seconds AS
SELECT
    key,
    date_trunc('minute', queries.time1) + x * INTERVAL '1 second' as time1,
    date_trunc('minute', queries.time1) + (x + 1) * INTERVAL '1 second' as time2
FROM
    queries,
    generate_series(0, 59) x
WHERE
    queries.time1 <= date_trunc('minute', queries.time1) + x * INTERVAL '1 second' AND
    queries.time2 >= date_trunc('minute', queries.time1) + (x + 1) * INTERVAL '1 second';
```
In case you read SQL as well as I do, what's going on here is that we pull out the minute of `time1`, and try out the 60 one second intervals after it.
Each interval is kept only if it comes strictly after `time1`, and strictly before `time2`.

It's actually not that complicated, computationally (it is a `flat_map` in Materialize).
The logic generalizes to minutes, hours, etc., and can even be used on the way back down if you round from `time2`.

You'll then need to take all of these intervals and union them together
```sql
-- union together derived "aligned" intervals.
CREATE VIEW query_intervals AS
SELECT * FROM time1_seconds UNION
SELECT * FROM time1_minutes UNION
SELECT * FROM hours UNION
SELECT * FROM time2_minutes UNION
SELECT * FROM time2_seconds;
```
You'll notice I've invented `hours` here.
I'll leave that as homework for you.
It's also worth stressing that I used `UNION` rather than `UNION ALL`.
There can be repetition if e.g. `time1` and `time2` are within the same hour (or minute).