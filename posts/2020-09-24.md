# Materialize under the Hood

Today we will take a bit of a tour of the moving parts that make up Materialize.
This tour isn't meant to be exhaustive, but rather to show off some of the moments where things might be different from what you expect, and to give you a sense for why Materialize is relatively better at maintaining SQL queries over changing data.


## In Broad Strokes

Let's start with some broad strokes that name the moving parts of the system.

[PICTURE]

Users interact with Materialize primarily through a `pgwire`-esque **external interface**, with commands that create, query, and remove data sources, views, and materializations.
These commands make their way to the **coordinator**, whose job it is to track the metadata about these sources, views, and materializations, and to communicate with the dataflow layer about what it should be doing next.
The **dataflow layer** is responsible for the execution and maintenance of the views the coordinator provides it.

We'll dive in to each of these in increasing detail.
Mostly, they are increasingly different from existing infrastructure, and so there is more that needs to be said as we go deeper down.
I'm also more familiar with the deeper down stuff myself; we'll need to get some other folks to explain

## External Interfaces

We have intentionally aimed to make Materialize's external interfaces as *uninteresting* as possible.
The more conventional these interfaces, the more existing tools and use cases can directly connect to Materialize and get moving, without new SDKs or programming effort.
You can use [`psql`](https://www.postgresql.org/docs/9.2/app-psql.html) to directly connect to Materialize, or BI tools like [Metabase](https://www.metabase.com) for an enriched experience.
From the outside, we want Materialize to look and feel like the database you and your data infrastructure expect.

Concretely, this means that your (or tools on your behalf) establish sessions, in which you CREATE, SELECT from, TAIL, and DROP various objects.
You can read about the full vocabulary in [the Materialize documentation](https://materialize.io/docs/).
Your session will pass your commands along to **the coordinator**, who determines whether they make sense, and what sort of response to provide.

## The Coordinator

Just behind the external interfaces lives the coordinator, the brains of Materialize.
This is where we track information about the sources of data you (and other users) have installed, views you have created over them, and the materializations we maintain for you.
Here we parse, plan, and optimize your SQL queries, and when appropriate instruct the dataflow layer to spin up a new computation to execute and maintain their results.
The coordinator also tracks the state of materializations, and ensures that we take advantage of them when planning how to answer and maintain new queries.

When your queries arrive they are little more than SQL text, and need to be parsed, planned, and optimized.
This process is largely well-understood, if fraught with semantic peril, but there are some quirks that are Materialize-specific.
The cost of *maintaining* a dataflow can be very different from the cost of executing a query once.
Materialize queries will be executed as long-lived stateful dataflow, and impose an ongoing cost of computation and memory.
As input data change, we want to quickly respond and cannot afford a full query re-evaluation.
This results in an optimization process that has different priorities than traditional optimizers.

The coordinator is also responsible for tracking the properties of the materializations we maintain.
Materializations are *of* some collections of data, and they are arranged *by* some keys (often columns).
These two characteristics tell the coordinator whether a materialization can assist in the construction and maintenance of a new dataflow.
The use and re-use of materialized data lie at the heart of what makes Materialize different from existing systems.

Ultimately, the main role of the coordinator is to provide instruction to **the dataflow layer**, which is where the dataflow computations for queries are assembled and maintained, and is where the data backing the materializations are house.

## Dataflow Execution

If the coordinator is the brains that thinks about what data processing to do, the dataflow layer is the muscle that makes it happen.
The dataflow layer houses the main departures from standard relational databases, and from the stream processing engines you might be most familiar with.

Our dataflow layer is built over [timely dataflow](https://github.com/TimelyDataflow/timely-dataflow) and [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow), scalable data-parallel dataflow frameworks designed to process information as efficiently as we know how.
Significantly, these frameworks are designed to capture and share state across multiple dataflows, using a tool called [shared arrangements](http://www.vldb.org/pvldb/vol13/p1793-mcsherry.pdf): the streaming dataflow equivalent of relational database indexes.
We will explain what these are, and connect the dots to maintaining SQL queries over changing data.

In timely dataflow, multiple worker threads cooperate to execution and maintain multiple dataflows.
Each worker thread knows about all dataflows, and can perform the logic for any of the operators; the routing of data to individual workers determines where the work actually occurs and where state is held.
This is a departure from many big data systems, which isolate the execution of each operator.

This departure is intentional. First, the design decouples the complexity of your queries from the complexity of your deployment. You can maintain hundreds of queries on a single machine, and scale up to more machines only when you want to. Second, the design allows our worker threads to share worker-local state and computation across dataflows. This brings the database concept of a shared index to the world of streaming computation, and with it interactive analysis of always fresh data.

[PICTURE]

A dataflow computation describes what should happen in response to the arrival of more data. In our case, the "data" will be **updates** to the underlying collections. Updates enter the system from data "sources", and flow forward through operators that implement traditional data-parallel logic: operators like `map`, `filter`, `join`, and `reduce`. Each of these operators are implemented to respond to input updates with the correct corresponding output updates: those that change the output to always reflect the correct answer for the accumulated input. Chaining these operators one after another results in a larger dataflow that maintains a more complex computation: one that can correspond to a SQL query!

[PICTURE]

All updates in Materialize bear a *logical timestamp*, an unambiguous indication of when the update takes place. All operators preserve this logical timestamp, and thereby maintain a consistent view of the results. Query results are never out of sync with one another, even though their execution is asynchronous and across multiple parallel workers. The logical timemstamps ensure that the results can be correctly collated before they are presented to users.

Some of these dataflow operators require *state*. For example, to respond to new updates the `join` operator needs access to the *history* of its input updates: each new update may match arbitrary records in the other input. In fact, the state needed by many operators is just an indexed representation of their input data.

In a relational setting, across dataflows many operators ultimately need the *same* state. Each time you join in a collection by its primary key, your dataflow could use that collection arranged by that key. Across all of your queries, there are often relatively fewer distinct ways data are used than there are uses of it. Shared arrangements are the mechanism that allows the operators to share the same in-memory state, even as its contents change. They are what make Materialize especially well-suited to maintain relational SQL queries over continually changing collections of data.

## In Conclusion

Materialize is a system designed to maintain relational queries over continually changing data. It has several specific advantages over other systems that make it better suited to this task. Specifically, Materialize is well-equiped to maintain indexed representations of collections of data as they change, and to leverage these indexes in maintaining queries over relational data.