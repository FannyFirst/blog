
# A Streaming Database

Materialize is a bit of an odd duck.
Folks rightly ask "are you a database or a stream processor?"
Let's dive in to that question, and unpick the distinctions between the two.
There doesn't need to be as much space between the two categories as you might think.

## What distinguishes a database from a stream processor?

Let's start with some similarities.
Both databases and stream processors mediate access to continually changing data.

That's about as far as I got.

Here's one take on their essential difference (I have several, but this is one):
The database provides value by imposing structure on and maintaining **data**.
The stream processor provides value by imposing structure on and maintaining **computations**.
They each tend to be less good at the other task.

But, if you are anything like the folks who have been reaching out to us at Materialize, you probably want both anyhow.

Let's look at three classic examples for reasons folks reach for streaming processors instead of databases:

1. People reach for stream processors when they have an unbounded source of data, and it just won't fit in a database.
2. People reach for stream processors when they want control over which data their queries reflect any time.
3. People reach for stream processors when they want to build event-driven architectures.

In these cases, the data itself is not as important as *reactions to* the data.
Obviously, there are other cases where the data are the more important thing.

## SQL over streaming data

I'm going to argue that SQL is a fine language for describing a large class of streaming tasks.
More specifically: *maintaining SQL queries over changing data* addresses a large class of streaming needs.
It's not going to solve all of your streaming problems, in the same way that I have graph computations that SQL can't handle.

But let's hit the three examples from above, each done up with just SQL.

We'll start with two example tables, `sensors` a dimension table containing information about some sensors we have, and `readings` a fact table containing all sensor measurements.

```sql
-- Information about each sensor
CREATE TABLE sensors (
    sensor_pk bigint,   -- id of the sensor (primary key)
    region text,        -- grouping of sensors
    model text          -- name of sensor
);

-- Measurements from the field
CREATE TABLE readings (
    sensor_fk bigint,   -- id of the sensor (foreign key)
    time_ms numeric,    -- time of the measurement
    temp real           -- the measurement itself
);
```
We are mainly going to be interested in pulling out the maximum `temp` grouped by `region`.
Both `temp` and `region` are in different relations, so we'll need to join them before aggregating.
```sql
SELECT sensor, time.hour, MAX(temperature)
FROM readings
GROUP BY sensor, time.hour
```

We will dress this up a bit as we go, but it's a pretty elementary query that nonetheless shows off some interesting problems and fixes, all without departing from SQL.

---

### Aggregations over unbounded data

People reach for stream processors when they have an unbounded source of data, and it just won't fit in a database.
Or perhaps it will fit, but it is so clearly inefficient or uneconomical to do so.

Let's say `readings` grows unboundedly with time, as many fact tables do.
You want to track the maximum temperature of each `sensor` by hour.
This is a pretty easy piece of SQL to knock out:
```sql
-- Maximum temperature for each sensor, each hour.
SELECT sensor, time.hour, MAX(temperature)
FROM readings
GROUP BY sensor, time.hour
```

Of course, you might be scared to run this because you can't fit `readings` into your database.
Or perhaps it just feels wrong to store per-second readings just to maintain hourly aggregates.
Let's not even get to the problem of re-reading all of this data each time you want to see these aggregates.
In any case, you aren't interested in maintaining all of this *data*, just *a view over the data*.

A stream processor isn't obliged to maintain your data for you.
It only needs to maintain the *query results*, rather than the input data.
The input data can be streamed in from cheaper storage, and once you are sure they are reflected, potentially discarded.
The concept is the same as "foreign tables" in databases: an external source of data whose contetns are managed elsewhere.

In Materialize, you can create *append-only* sources, which will have more efficient implementations for queries like the one above.
We'll maintain the current results as they change, and not the whole input data.

Crucially, you don't have to write any weird SQL to get that to happen.

![Unbounded data example](https://github.com/frankmcsherry/blog/blob/master/assets/sqlability/unbounded_data.svg)


---

### Windowing queries by time

People reach for stream processors when they want control over which data their queries reflect any time.
Concretely, folks often want the ability to focus on recent data and age-out old data.

A classic example is the query we saw above, which grouped measurements by hour.
Why not instead maintain the maximum over just the past hour's data?
Something like:

```sql
-- Maximum temperature for each sensor, over the past hour's data.
SELECT sensor, MAX(temperature)
FROM readings
WHERE CURRENT_TIME() BETWEEN time_ms AND time_ms + '1 HOUR'
GROUP BY sensor
```

This query uses the `CURRENT_TIME()` SQL method which returns the current system time, and compares it against `readings.time_ms`.

If you issue this query, the system will first evaluate `CURRENT_TIME()` and then look for records that satisfy the predicate.
If you continually re-execute the query, the results will change as `readings` change, but also as `CURRENT_TIME()` changes.
Records will appear as soon as `CURRENT_TIME()` is greater than their inserted `time_ms`, and they will vanish an hour later when `CURRENT_TIME()` is no longer less than `time_ms + '1 HOUR'`.

While you can probably guess how to *compute* the query, it is also surprisingly easy to *maintain*.
The query itself tells us when a given reading comes in to effect (its `time_ms`), and when it should be retracted (its `time_ms + '1 HOUR'`)

By refering to SQL-standard `CURRENT_TIME()` in your query, you describe

*caveat*: In Materialize you must use `mz_logical_time()` instead of `CURRENT_TIME()`.
The former references the event time of your data, which is often but not always the same reckoning as the time experienced by the system.

![Temporal filter example](https://github.com/frankmcsherry/blog/blob/master/assets/sqlability/temporal_filter.svg)


---

### Prompting queries by events

People reach for stream processors when they want to build event-driven architectures.
Rather than continually poll databases, they would like to be notified when new information is available, or when old information changes.

Most databases have a concept of a "prepared statement": A query with "holes" that can execute once these parameters are supplied.
Prepared statements are a great way to repeatedly poll your database with the same class of query, efficiently.
But it turns out SQL already has a mechanism for this in the query language, you just might not recognize it.

A `LATERAL JOIN` is between two collections, and exposes the columns of the first collection to the second, which can be a correlated subquery.
The first collection can just contain your paramater bindings, and the subquery can be your prepared statement, using the available bindings.

Let's imagine you have your sensor readings above, and on request you'd like to report the top few readings for each location.
It's rather expensive to keep the top few readings up to date for all locations, though you could certainly do that.
But, you could also write the following LATERAL JOIN between a collection `queries` of locations of interest, and a subquery that pulls out the top few maximum readings:

```sql
-- Three sensors with hottest readings in the past hour,
-- for each of the queried locations.
SELECT location, sensor, max_temp
FROM
    queries,
    LATERAL (
        -- Top three sensors in `queries.location`.
        SELECT sensor, MAX(temperature) AS max_temp
        FROM readings,
        WHERE now() BETWEN time AND time + 1 HOUR
          AND readings.location = queries.location
        GROUP BY sensor
        ORDER BY max_temp
        LIMIT 3
    )
```

As long as `queries` is empty this query's results will be empty too.
As soon as you add a record to `queries`, some location you are interested in, the output will change to include the top three sensors for that location.
As long as your location is in `queries`, you'll see any changes to the top three results for your location.
As soon as you remove your location from `queries` the results will be removed and you'll see no more updates.

The LATERAL JOIN pattern allows you to set up parameterized queries, and enable and disable change tracking just by supplying data.

The best example I've heard of this comes from one of our users.
They have a workflow where their analysts show up with a problematic ID, and want to crack open a standard set of dashboards on data related to this ID.
They could set up those queries by hand, or perhaps with some scripts, but a few `LATERAL JOIN` queries allow someone else to define and automate the view set-up; the analyst just needs to add the ID to the `queries` collection to start things up, and drop the ID once they are done.

![Lateral join example](https://github.com/frankmcsherry/blog/blob/master/assets/sqlability/lateral_join.svg)


##

