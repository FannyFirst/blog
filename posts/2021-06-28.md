
# A Streaming Database

Materialize is a bit of an odd duck.
Folks rightly ask "are you a database or a stream processor?"
Let's dive in to that question, and unpick the distinctions between the two.
There doesn't need to be as much space between the two categories as you might think.

## What distinguishes a database from a stream processor?

Let's start with some similarities.
Both databases and stream processors mediate access to continually changing data.

That's about as far as I got.

The database provides value by imposing structure on and maintaining **data**.
The stream processor provides value by imposing structure on and maintaining **computations**.
They each tend to be less good at the other task, or if you want to do their speciality your way instead.

But, if you are anything like the folks who have been reaching out to us at Materialize, you probably want both anyhow.

Let's look at three classic examples for why folks reach for streaming processors instead of databases:

1. People reach for stream processors because they have an unbounded source of data, and it just won't fit in a database.
2. People reach for stream processors when they want control over which data their queries reflect any time.
3. People reach for stream processors because they want to build event-driven architectures.

In these cases, the data itself is not as important as *reactions to* the data.
Obviously, there are other cases where the data are the more important thing.

## SQL over streaming data

I'm going to argue that SQL is a fine language for describing a large class of streaming tasks.
More specifically: *maintaining SQL queries over changing data* addresses a large class of streaming needs.
It's not going to solve all of your streaming problems, in the same way that I have graph computations that SQL can't handle.

### Aggregations over unbounded data

People reach for stream processors because they have an unbounded source of data, and it just won't fit in a database.
Or perhaps it will fit, but it is so clearly inefficient or uneconomical to do so.

Let's say you have a continually growing collection of `(sensor, temperature, time)` measurements.
You want to track the maximum temperature of each `sensor` by hour.
This is a pretty easy piece of SQL to knock out:
```sql
-- Maximum temperature for each sensor, each hour.
SELECT sensor, time.hour, MAX(temperature)
FROM readings
GROUP BY sensor, time.hour
```

Of course, you might be scared to run this because you can't fit `readings` into your database.
Or perhaps it just feels wrong to store per-second readings just to maintain hourly aggregates.
Let's not even get to re-reading all of this data each time you want to see these aggregates.
In any case, you aren't interested in maintaining all of this *data*, just *a view over the data*.

A stream processor isn't obliged to maintain your data for you.
It only needs to maintain the *query results*, rather than the input data.
The input data can be streamed in from cheaper storage, and once you are sure they are reflected, potentially discarded.

In Materialize, you can create append-only sources, which will have more efficient implementations for queries like the one above.
We'll maintain the current results as they change, and not the whole input data.

### Windowing queries by time

People reach for stream processors when they want control over which data their queries reflect any time.
Specifically, folks often want the ability to focus on recent data and age-out old data.

A classic example is the query we saw above, which grouped measurements by hour.
Why not maintain the maximum over just the past hour's data?
Something like:

```sql
-- Maximum temperature for each sensor, over the past hour's data.
SELECT sensor, MAX(temperature)
FROM readings
WHERE now() BETWEEN time AND time + '1 HOUR'
GROUP BY sensor
```

This query uses the `now()` method which returns the current system time.
If you issue this query, the system will first evaluate `now()` and then look for records that satisfy the predicate.
The query results change both as `readings` change, but also as `now()` changes.

While you can probably guess how to *compute* the query, it is also surprisingly easy to *maintain*.
The query itself tells us when a given reading comes in to effect (its `time`), and when it should be retracted (its `time + '1 HOUR'`).

*caveat*: In Materialize you should use `mz_logical_time()` instead of `now()`.

### Prompting queries by events

People reach for stream processors because they want to build event-driven architectures.
Rather than continually poll databases, they would like to be notified when new information is available, or when old information changes.

Most databases have a concept of a "prepared statement": A query with "holes" that can execute once these parameters are supplied.
Prepared statements are a great way to repeatedly poll your database with the same class of query, efficiently.
But it turns out SQL already has a mechanism for this in the query language, you just might not recognize it.

A `LATERAL JOIN` is between two collections, and exposes the columns of the first collection to the second, which can be a correlated subquery.
The first collection can just contain your paramater bindings, and the subquery can be your prepared statement, using the available bindings.

Let's imagine you have your sensor readings above, and on request you'd like to report the top few readings for each location.
It's rather expensive to keep the top few readings up to date for all locations, though you could certainly do that.
But, you could also write the following LATERAL JOIN between a collection `queries` of locations of interest, and a subquery that pulls out the top few maximum readings:

```sql
-- Three sensors with hottest readings in the past hour,
-- for each of the queried locations.
SELECT location, sensor, max_temp
FROM
    queries,
    LATERAL (
        -- Top three sensors in `queries.location`.
        SELECT sensor, MAX(temperature) AS max_temp
        FROM readings,
        WHERE now() BETWEN time AND time + 1 HOUR
          AND readings.location = queries.location
        GROUP BY sensor
        ORDER BY max_temp
        LIMIT 3
    )
```

As long as `queries` is empty this query's results will be empty too.
As soon as you add a record to `queries`, some location you are interested in, the output will change to include the top three sensors for that location.
As long as your location is in `queries`, you'll see any changes to the top three results for your location.
As soon as you remove your location from `queries` the results will be removed and you'll see no more updates.

The LATERAL JOIN pattern allows you to set up parameterized queries, and enable and disable change tracking just by supplying data.

The best example I've heard of this comes from one of our users.
They have a workflow where their analysts show up with a problematic ID, and want to crack open a standard set of dashboards on data related to this ID.
They could set up those queries by hand, or perhaps with some scripts, but a few `LATERAL JOIN` queries allow someone else to define and automate the view set-up; the analyst just needs to add the ID to the `queries` collection to start things up, and drop the ID once they are done.

