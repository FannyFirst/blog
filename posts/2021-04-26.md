# Generalizing linear operators

Differential dataflows contain many operators, some of which are very complicated, but many of which are relatively simple.

The `map` operator applies a record-by-record transformation, and has a three line implemenation.
The `filter` operator applies a predicate to each record, and drops records that do not pass it.
The `flat_map` operator applies a map to each record that can result in any number of output records.

These three methods are all generalized by the `flat_map` method, which you may be able to see with a bit of head scratching.

There are a few more linear operators, slightly more complicated ones, and it turns out that they can be generalized as well.
In this post we'll work through these more complicated, and very interesting, linear operator and generalize them.
We'll end with some discussion of the implications for Materialize, which unlike differential dataflow has the ability to optimize these operators.

## Differential dataflow background

Differential dataflow acts on *streams of updates*, where each update is a triple
```
(data, time, diff)
```
The `data` component describes *where* the update occurs: which record is experiencing the change.
The `time` component describes *when* the update occurs: at which moment should the change take effect.
The `diff` component describes *what* the update changes: commonly, an integer for inserts or deletes.

These triples describe the history of changes to a collection of records.
Any changing collection can be recorded as a stream of these updates, just by subtracting from each collection the prior collection.

Operators act on streams of updates, and their jobs are to transform these streams into new output streams of updates.
The `map` operator takes changes for one collection and produces the changes for a collection in which each record is subjected to some expression.
The `filter` operator takes changes for one collection and produces the changes for the subset of records that satisfy the predicate.
The `join` operator takes changes  for two collections and produces the changes for the collection that pairs up records with matching keys.
Operator implementations can be weird and interesting, but their intended behavior is meant to be pretty clear.

## Linear operators

Some of our operators have the mathematical property of "linearity".
```
OP(x + y) = OP(x) + OP(y)
```
Linearity means that the operator can be applied record by record if we want.

Let's look at an example: the `filter` operator.
The `filter` operator takes a `predicate` argument and applies it to each record, producing either the record as output (if it passes) and producing nothing (if it fails).
The `filter` operator is linear: the predicate can be applied independently to each record, and the independent results then combined.

The main exciting thing about a linear operator is that it gives us a pretty easy differential dataflow operator implementation.
For any `data`, our linear operator applied to `{ data }` produces some collection `{ datum1, datum2, .. }` of output.
Now for any `(data, time, diff)` update triple, we produce as output the updates
```
(datum1, time, diff)
(datum2, time, diff)
..
```

It turns out this is a correct operator implementation!
It's also pretty easy to implement, and keeps our `map`, `filter`, and `flat_map` operators simple and performant.
Each of those operators are also linear, which you can double check if you like!

## Even more linear operators

As it turns out, there are some other interesting operators out there.
Linear operators!

Differential dataflow has an `explode` operator, which is a too-exciting name for an operator that is allowed to produce output `diff` information.
The `explode` operator maps each `data` to an iterator over `(value, diff)` all of which it then produces for each input.
The original intent might be that you'd have accumulations `(key, count)` that you might want to turn in to `count` copies of `key`.
The `explode` operator would let you do this efficiently, without actually producing `count` actual copies of `key` (perhaps `count` is enormous).
But, the operator is also really interesting because it can produce negative `diff` values, turning a positive record into a negative (and vice versa).

Materialize has a conncept of "temporal filter" which is able to transform constraints between `data` and `time` in to an operator that adjusts `time`.
Concretely, if you say that `time` must live between `lower(data)` and `upper(data)` then the operator can replace each `data` by the updates
```
(data, lower(data), +1)
(data, upper(data), -1)
```
These updates defer the introduction of `data` until `lower(data)` and retract `data` at `upper(data)`.

These implementations of these two operators are a bit more subtle than the easier linear operators up above.
The `explode` operator needs to be sure to *multiply* the input `diff` with the produced `diff`.
The temporal filter operator needs to be sure to take the *maximum* of the input `time` with those produced by `lower` and `upper`.
Each of these operations requires care in their implementation, and things are certainly becoming more complicated.

## All of the linear operators

All of the operators above, and indeed all linear operators, are instances of one most general linear operator.

Let `logic` be any function from `data` to a time-varying collection (one of those things defined by a set of update triples).
Let `LARGE` be the collection containing the sum over all `data` of the collection `data x logic(data)`.
The operator that performs an equijoin between its input and `LARGE` is a linear operator.
If you project away the `data` component, you can represent any linear operator by your choice of `logic`.

Let's work through some examples.

1. `map(f)`: let `logic(data)` be the collection that always contains exactly `f(data)`.

2. `filter(p)`: let `logic(data)` be the collection that either contains exactly `data` or is empty, based on `p(data)`.

3. `flat_map(f)`: let `logic(data)` be the collection that always contains exactly the collection `f(data)`.

4. `explode(f)`: let `logic(data)` be the collection that is defined by the set of `(value, diff)` from `f(data)` at all times.

5. temporal filters: let `logic(data)` be the collection that contains `data` exactly from `lower(data)` until `upper(data)`.

In each of these cases, we join our input collection with `LARGE` and then project away `data`.

## An implementation

This operator has a simple implementation, though one that I find harder to justify verbally.
For a timely dataflow stream of `(data, time, diff)` update triples, we can use timely's `flat_map` operator to react to each of these triples.
```rust
// Linear operator on a stream of update triples.
// Parameterized by the function `logic`.
self.flat_map(move |(data, time, diff)|
    logic(data)
        .into_iter()
        .map(move |(data2, time2, diff2)|
            (
                data2,                  // new `data2`
                time.join(&time2),      // joined times
                diff.multiply(&diff2),  // multiplied diffs
            )
        )
)
```
For each `data`, we enumerate `logic(data)`, and produce new output updates.
The updates have the newly enumerated data, each at the time that is `time` and `time2` merged by the lattice join operator, and with `diff` and `diff2` merged by multiplication.

You can also check out the (new) operator `join_function` in [the differential dataflow repository](https://github.com/TimelyDataflow/differential-dataflow), where it looks like (with all of the gory Rust details):
```rust
/// Joins each record against a collection defined by the function `logic`.
///
/// This method performs what is essentially a join with the collection of records `(x, logic(x))`.
/// Rather than materialize this second relation, `logic` is applied to each record and the appropriate
/// modifications made to the results, namely joining timestamps and multiplying differences.
///
/// # Examples
///
/// ```
/// extern crate timely;
/// extern crate differential_dataflow;
///
/// use differential_dataflow::input::Input;
///
/// fn main() {
///     ::timely::example(|scope| {
///         // creates `x` copies of `2*x` from time `3*x` until `4*x`,
///         // for x from 0 through 9.
///         scope.new_collection_from(0 .. 10isize).1
///              .join_function(|x|
///                  //   data      time      diff
///                  vec![(2*x, (3*x) as u64,  x),
///                       (2*x, (4*x) as u64, -x)]
///               );
///     });
/// }
/// ```
pub fn join_function<D2, R2, I, L>(&self, mut logic: L) -> Collection<G, D2, <R2 as Multiply<R>>::Output>
where G::Timestamp: Lattice,
        D2: Data,
        R2: Semigroup+Multiply<R>,
        <R2 as Multiply<R>>::Output: Data+Semigroup,
        I: IntoIterator<Item=(D2,G::Timestamp,R2)>,
        L: FnMut(D)->I+'static,
{
    self.inner
        .flat_map(move |(x, t, d)| logic(x).into_iter().map(move |(x,t2,d2)| (x, t.join(&t2), d2.multiply(&d))))
        .as_collection()
}
```

## Implications for Materialize

Materialize is among other things a declarative SQL layer on top of differential dataflow.

By being declarative, Materialize has the ability to restructure the queries it receives.
In particular, it is delighted to take stacks of `Map`, `Filter`, and `Project` actions and fuse them together.
This is exceedingly helpful because these linear operators can be fused in to operators like `Join`, among others, where they can substantially reduce the volume of data stored and moved around.

However, Materialize stalls out on anything more copmlicated than the three operations above.
Until very recently, it also stalled out on temporal filters, though through some care these can now be fused as well.
In fact, Materialize has a great number of special purposed "table valued functions" which are used to implement `flat_map`-like behavior.
For example, you might type something like
```sql
SELECT *
FROM
    my_data,
    generate_series(1, my_data.count);
```
Here `generate_series` is the table valued function, and it is even used as a join!

Materialize has a few other tricks that end up with similar situations.
The `repeat_row` table valued function can produce negative rows as output, which means it is more `explode` than `flat_map`.
The temporal filters were special-cased but still feel msore like these joins, for the reasons mentioned above.
These cases all live outside the framework of `Map`, `Filter`, and `Project`.

So I'm thrilled by the idea that all of these concepts might be unified up into one framework.
That unified representation could then be optimized, and fused in to other operators.
For those of you using temporal filters, this would allow them to be better pushed down in to joins, and it can reduce their memory footprint substantially in some cases.
Internally, some of our CDC format unpacking uses this logic, and jointly optimizing that logic with the SQL you have layered on top of it gives us the ability to unpack and manipulate less.

All in all, I'm excited that we might end up reducing the number of concepts that we work with, simplifying things at the same time as we open up new doors for performance.